{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "import scipy\n",
    "from scipy.stats import norm\n",
    "\n",
    "from rapidfuzz.distance import Levenshtein\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import pickle\n",
    "import joblib\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "import sys \n",
    "import random\n",
    "import string\n",
    "import math\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Version Checks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python: 3.12.9 (tags/v3.12.9:fdb8142, Feb  4 2025, 15:27:58) [MSC v.1942 64 bit (AMD64)]\n",
      "\n",
      "numpy: 2.2.4\n",
      "pandas: 2.2.3\n",
      "\n",
      "scipy: 1.15.2\n",
      "\n",
      "matplotlib: 3.10.1\n",
      "seaborn: 0.13.2\n"
     ]
    }
   ],
   "source": [
    "print('python:', sys.version)\n",
    "print()\n",
    "\n",
    "print('numpy:', np.__version__)\n",
    "print('pandas:', pd.__version__)\n",
    "print()\n",
    "\n",
    "print('scipy:', scipy.__version__)\n",
    "print()\n",
    "\n",
    "print('matplotlib:', matplotlib.__version__)\n",
    "print('seaborn:', sns.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Template class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "import numpy as np\n",
    "\n",
    "class TreeTemplate(ABC):\n",
    "    def __init__(self, num_strings, split_num, depth, s1_idx=None, s2_idx=None, tf_array=None):\n",
    "        self.num_strings = num_strings\n",
    "        self.split_num = split_num\n",
    "        self.depth = depth\n",
    "        self.s1_idx = s1_idx\n",
    "        self.s2_idx = s2_idx\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "\n",
    "        if tf_array is None and depth == 0:\n",
    "            self.tf_array = np.zeros((num_strings, split_num), dtype=bool)\n",
    "        else:\n",
    "            self.tf_array = tf_array\n",
    "\n",
    "    @abstractmethod\n",
    "    def distance(self, a, b):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def build_tree(self, strings, indices):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def find_matches(self, strings, new_str):\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def save_tree(self, filename):\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    @abstractmethod\n",
    "    def load_tree(filename):\n",
    "        pass\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "\n",
    "class IndexTemplate(ABC):\n",
    "    def __init__(self, num_trees, num_strings, split_num):\n",
    "        self.num_trees = num_trees\n",
    "        self.num_strings = num_strings\n",
    "        self.split_num = split_num\n",
    "        self.trees = []\n",
    "\n",
    "    @abstractmethod\n",
    "    def add_item(self, i, item):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def build(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def unbuild(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_nns_by_vector(self, query, topk=None):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_nns_by_item(self, i, n=None):\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def save(self, filename):\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    @abstractmethod\n",
    "    def load(filename):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def unload(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def on_disk_build(self, fn) -> bool:\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ForestTemplate(ABC):\n",
    "    def __init__(self, n_estimators, max_depth, random_state=None):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.random_state = random_state\n",
    "        self.trees = []\n",
    "        self.strings = []\n",
    "        if random_state is not None:\n",
    "            random.seed(random_state)\n",
    "            np.random.seed(random_state)\n",
    "        self._is_fitted = False \n",
    "\n",
    "\n",
    "    @abstractmethod\n",
    "    def fit(self, strings: list[str]):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def transform(self, strings: list[str]) -> np.ndarray:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict(self, queries: list[str], topk=1) -> list[list[str]]:\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def save(self, filename: str):\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    @abstractmethod\n",
    "    def load(filename: str):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_params(self, deep=True):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def set_params(self, **params):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def __repr__(self):\n",
    "        pass\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LevenshteinTree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rapidfuzz.distance import Levenshtein\n",
    "import pickle\n",
    "\n",
    "class LevenshteinTree(TreeTemplate):\n",
    "    def __init__(self, num_strings, split_num, depth, s1_idx=None, s2_idx=None, tf_array=None, distance_fn=None):\n",
    "        super().__init__(num_strings, split_num, depth, s1_idx, s2_idx, tf_array)\n",
    "        self._distance_fn = distance_fn or Levenshtein.distance\n",
    "    \n",
    "    def distance(self, a, b):\n",
    "        return self._distance_fn(a, b)\n",
    "\n",
    "    def build_tree(self, strings, indices):\n",
    "        # len(indices) == 0 or 1, can't split more\n",
    "        if self.depth >= self.split_num or len(indices) <= 1:\n",
    "            return None  # no node needed for empty/terminal group\n",
    "\n",
    "        # len(indices) == 2, just split in two\n",
    "        if len(indices) == 2:\n",
    "            self.tf_array[indices[0], self.depth] = True\n",
    "            self.tf_array[indices[1], self.depth] = False\n",
    "            return LevenshteinTree(self.num_strings, self.split_num, self.depth)\n",
    "\n",
    "        # Most cases\n",
    "        rand_pos = np.random.choice(len(indices), size=2, replace=False)\n",
    "        s1_idx = indices[rand_pos[0]]\n",
    "        s2_idx = indices[rand_pos[1]]\n",
    "\n",
    "        node = LevenshteinTree(self.num_strings, self.split_num, self.depth, s1_idx, s2_idx, self.tf_array)\n",
    "\n",
    "        for idx in indices:\n",
    "            d1 = self.distance(strings[idx], strings[s1_idx])\n",
    "            d2 = self.distance(strings[idx], strings[s2_idx])\n",
    "            self.tf_array[idx, self.depth] = (d1 >= d2)\n",
    "\n",
    "        mask = self.tf_array[indices, self.depth]\n",
    "        left = indices[np.flatnonzero(mask)]\n",
    "        right = indices[np.flatnonzero(~mask)]\n",
    "\n",
    "        node.left = LevenshteinTree(self.num_strings, self.split_num, self.depth + 1, tf_array=self.tf_array).build_tree(strings, left)\n",
    "        node.right = LevenshteinTree(self.num_strings, self.split_num, self.depth + 1, tf_array=self.tf_array).build_tree(strings, right)\n",
    "\n",
    "        return node\n",
    "\n",
    "    def get_code(self, strings, new_str):\n",
    "        fingerprint = np.zeros(self.split_num, dtype=bool)\n",
    "        idx = 0\n",
    "        node = self\n",
    "\n",
    "        while node and node.s1_idx is not None and node.s2_idx is not None:\n",
    "            d1 = self.distance(new_str, strings[node.s1_idx])\n",
    "            d2 = self.distance(new_str, strings[node.s2_idx])\n",
    "            go_left = d1 >= d2\n",
    "            fingerprint[idx] = go_left\n",
    "            node = node.left if go_left else node.right\n",
    "            idx += 1\n",
    "            \n",
    "            # just in case\n",
    "            assert idx < self.split_num, f\"Fingerprint overflowed! idx={idx}, split_num={self.split_num}\"\n",
    "\n",
    "\n",
    "        return fingerprint\n",
    "\n",
    "    def find_matches(self, strings, new_str, return_strings=True):\n",
    "        fingerprint = self.get_code(strings, new_str)\n",
    "        matches = np.where((self.tf_array == fingerprint).all(axis=1))[0]\n",
    "\n",
    "        if return_strings:\n",
    "            return matches, [strings[i] for i in matches]\n",
    "        else:\n",
    "            return matches\n",
    "        \n",
    "    def transform(self, strings: list[str]) -> np.ndarray:\n",
    "        result = np.zeros((len(strings), self.split_num), dtype=bool)\n",
    "        for i, s in enumerate(strings):\n",
    "            result[i] = self.get_code(strings, s)\n",
    "        return result\n",
    "\n",
    "    def depth(self) -> int:\n",
    "        def _max_depth(node):\n",
    "            if node is None:\n",
    "                return 0\n",
    "            return 1 + max(_max_depth(node.left), _max_depth(node.right))\n",
    "\n",
    "        return _max_depth(self)\n",
    "\n",
    "    def save_tree(self, filename):\n",
    "        with open(filename, \"wb\") as f:\n",
    "            pickle.dump(self, f)\n",
    "\n",
    "    @staticmethod\n",
    "    def load_tree(filename):\n",
    "        with open(filename, \"rb\") as f:\n",
    "            return pickle.load(f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LevenshteinIndex\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LevenshteinIndex(IndexTemplate):\n",
    "    def __init__(self, num_trees, num_strings, split_num):\n",
    "        super().__init__(num_trees, num_strings, split_num)\n",
    "        self._string_buffer = [None] * num_strings  # reserve space\n",
    "        self._item_count = 0\n",
    "\n",
    "    # add one-by-one\n",
    "    def add_item(self, i: int, string: str):\n",
    "        if self._string_buffer[i] is None:\n",
    "            self._item_count += 1\n",
    "        self._string_buffer[i] = string\n",
    "\n",
    "    # add data at once\n",
    "    def add_items_bulk(self, strings):\n",
    "        if not isinstance(strings, (list, np.ndarray, pd.Series)):\n",
    "            raise TypeError(\"Input must be a list, numpy array, or pandas Series of strings.\")\n",
    "\n",
    "        strings_array = np.asarray(strings, dtype=object)\n",
    "        n = len(strings_array)\n",
    "\n",
    "        if n > len(self._string_buffer):\n",
    "            raise ValueError(\"Too many strings to add to the index.\")\n",
    "\n",
    "        # Update item count for previously None entries\n",
    "        mask = np.array(self._string_buffer[:n], dtype=object) == None\n",
    "        self._item_count += np.count_nonzero(mask)\n",
    "\n",
    "        # Bulk assign to the internal buffer\n",
    "        self._string_buffer[:n] = strings_array\n",
    "\n",
    "\n",
    "    # Annoy style\n",
    "    def build(self):\n",
    "        strings = self._string_buffer\n",
    "        for i in range(self.num_trees):\n",
    "            root = LevenshteinTree(self.num_strings, self.split_num, 0)\n",
    "            tree = root.build_tree(strings, np.arange(self.num_strings))\n",
    "            self.trees.append(tree)\n",
    "\n",
    "    def unbuild(self):\n",
    "        self.trees = []\n",
    "        return True\n",
    "    \n",
    "\n",
    "    def get_nns_by_vector(self, query_str, topk=None):\n",
    "        all_matches = []\n",
    "\n",
    "        for tree in self.trees:\n",
    "            matches = tree.find_matches(self._string_buffer, query_str)\n",
    "            all_matches.extend(matches)\n",
    "\n",
    "        # Remove duplicates, keep unique indices\n",
    "        unique_matches = list(set(all_matches))\n",
    "\n",
    "        if topk:\n",
    "            # Compute Levenshtein distances to query_str for all unique matches\n",
    "            scored = [(idx, Levenshtein.distance(query_str, self._string_butter[idx])) for idx in unique_matches]\n",
    "            scored.sort(key=lambda x: x[1])  # sort by distance (ascending)\n",
    "            return [idx for idx, dist in scored[:topk]]\n",
    "\n",
    "        return unique_matches  # return unsorted matches if topk not requested\n",
    "    \n",
    "    # same as get_nns_by_vector\n",
    "    def get_nns_by_string(self, query_str, topk=None):\n",
    "        return self.get_nns_by_vector(query_str, topk=topk)\n",
    "\n",
    "    def get_nns_by_item(self, i, n=None):\n",
    "        query_str = self._string_buffer[i]\n",
    "        return self.get_nns_by_string(query_str, topk=n)\n",
    "\n",
    "\n",
    "    def save(self, filename: str):\n",
    "        with open(filename, \"wb\") as f:\n",
    "            pickle.dump(self, f)\n",
    "\n",
    "    @staticmethod\n",
    "    def load(filename: str):\n",
    "        with open(filename, \"rb\") as f:\n",
    "            return pickle.load(f)\n",
    "        \n",
    "    def unload(self):\n",
    "        self.trees = []\n",
    "        self._string_buffer = []\n",
    "        self._item_count = 0\n",
    "        return True\n",
    "    \n",
    "\n",
    "    def on_disk_build(self, fn: str) -> bool:\n",
    "        # Placeholder: no real on-disk building for Levenshtein trees\n",
    "        print(f\"[Warning] on_disk_build is not supported yet.\")\n",
    "        return True\n",
    "\n",
    "\n",
    "    def get_distance(self, i: int, j: int) -> int:\n",
    "        if not hasattr(self, '_string_buffer'):\n",
    "            raise ValueError(\"No string data loaded. Use add_item() or build().\")\n",
    "\n",
    "        return Levenshtein.distance(self._string_buffer[i], self._string_buffer[j])\n",
    "\n",
    "    def get_n_items(self) -> int:\n",
    "        return self._item_count\n",
    "    \n",
    "    def get_n_trees(self) -> int:\n",
    "        return len(self.trees)\n",
    "    \n",
    "    def get_strings(self) -> list[str]:\n",
    "        return self._string_buffer\n",
    "    \n",
    "    def get_item_vector(self, i: int, tree_id: int = 0) -> list[bool]:\n",
    "        if not self.trees:\n",
    "            raise ValueError(\"No trees built.\")\n",
    "        string = self._string_buffer[i]\n",
    "        return self.trees[tree_id].get_code(self._string_buffer, string).tolist()\n",
    "    \n",
    "\n",
    "    def verbose(self, v: bool) -> bool:\n",
    "        self._verbose = v\n",
    "        return True\n",
    "\n",
    "    def set_seed(self, s: int) -> None:\n",
    "        random.seed(s)\n",
    "        np.random.seed(s)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LevenshteinForest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LevenshteinForest(ForestTemplate):\n",
    "    def __init__(self, n_estimators=10, max_depth=120, random_state=None):\n",
    "        super().__init__(n_estimators, max_depth, random_state)\n",
    "        \n",
    "    def fit(self, strings: list[str]):\n",
    "        self.strings = strings\n",
    "        self.num_strings = len(strings)\n",
    "\n",
    "        self.trees = []\n",
    "        for _ in range(self.n_estimators):\n",
    "            root = LevenshteinTree(self.num_strings, self.max_depth, 0)\n",
    "            tree = root.build_tree(strings, np.arange(self.num_strings))\n",
    "            self.trees.append(tree)\n",
    "\n",
    "        self._is_fitted = True\n",
    "        return self\n",
    "\n",
    "    def transform(self, strings: list[str]) -> np.ndarray:\n",
    "        assert self._is_fitted, \"Call fit() before transform()\"\n",
    "\n",
    "        result = np.zeros((len(strings), self.n_estimators * self.max_depth), dtype=bool)\n",
    "\n",
    "        for i, s in enumerate(strings):\n",
    "            code_parts = []\n",
    "            for tree in self.trees:\n",
    "                code = tree.get_code(self.strings, s)\n",
    "                code_parts.append(code)\n",
    "            result[i] = np.concatenate(code_parts)\n",
    "\n",
    "        return result\n",
    "    \n",
    "    def predict(self, queries: list[str], topk=1) -> list[list[str]]:\n",
    "        assert self._is_fitted, \"Call fit() before predict()\"\n",
    "        predictions = []\n",
    "\n",
    "        for q in queries:\n",
    "            match_counts = {}\n",
    "\n",
    "            for tree in self.trees:\n",
    "                matches = tree.find_matches(self.strings, q, return_strings=False)\n",
    "                for idx in matches:\n",
    "                    match_counts[idx] = match_counts.get(idx, 0) + 1\n",
    "\n",
    "            # Sort matches by how often they appeared\n",
    "            sorted_matches = sorted(match_counts.items(), key=lambda x: -x[1])\n",
    "            top_indices = [idx for idx, _ in sorted_matches[:topk]]\n",
    "            top_strings = [self.strings[i] for i in top_indices]\n",
    "\n",
    "            predictions.append(top_strings)\n",
    "\n",
    "        return predictions\n",
    "    \n",
    "    import pickle\n",
    "    def save(self, filename: str):\n",
    "        with open(filename, \"wb\") as f:\n",
    "            pickle.dump(self, f)\n",
    "\n",
    "    @staticmethod\n",
    "    def load(filename: str):\n",
    "        with open(filename, \"rb\") as f:\n",
    "            return pickle.load(f)\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        return {\n",
    "            \"n_estimators\": self.n_estimators,\n",
    "            \"max_depth\": self.max_depth,\n",
    "            \"random_state\": self.random_state\n",
    "        }\n",
    "\n",
    "    def set_params(self, **params):\n",
    "        for key, value in params.items():\n",
    "            if hasattr(self, key):\n",
    "                setattr(self, key, value)\n",
    "        return self\n",
    "    \n",
    "\n",
    "    def __repr__(self):\n",
    "        return (\n",
    "            f\"LevenshteinForest(\"\n",
    "            f\"n_estimators={self.n_estimators}, \"\n",
    "            f\"max_depth={self.max_depth}, \"\n",
    "            f\"random_state={self.random_state}, \"\n",
    "            f\"is_fitted={self._is_fitted})\"\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Usage\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LevenshteinIndex\n",
    "\n",
    "- add string one by one from file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create index\n",
    "leven_index = LevenshteinIndex(num_trees=10, num_strings=10_000, split_num=120)\n",
    "\n",
    "# add strings to index\n",
    "with open('Data/Random_strings_10K.txt', 'r', encoding='utf-8') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        leven_index.add_item(i, line.strip())\n",
    "\n",
    "# build index\n",
    "leven_index.build()\n",
    "\n",
    "# query\n",
    "query = \"hello world\"\n",
    "neighbors = leven_index.get_nns_by_string(query, topk=3)\n",
    "\n",
    "print(\"Top 3 nearest neighbors:\")\n",
    "for idx in neighbors:\n",
    "    print(f\"[{idx}] {leven_index.get_strings()[idx]}\")\n",
    "\n",
    "\n",
    "leven_index.save(\"Models/LevenshteinIndex_10k_1.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LevenshteinIndex\n",
    "\n",
    "- just read all string at once\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Data/Random_strings_10K.txt', 'r', encoding='utf-8') as f:\n",
    "    strings = [line.strip() for line in f]\n",
    "\n",
    "# Create index and add\n",
    "leven_index = LevenshteinIndex(num_trees=10, num_strings=len(strings), split_num=120)\n",
    "# for i, s in enumerate(strings):\n",
    "#     leven_index.add_item(i, s)\n",
    "leven_index.add_items_bulk(strings)\n",
    "\n",
    "\n",
    "# build index\n",
    "leven_index.build()\n",
    "\n",
    "# Query\n",
    "query = \"hello world\"\n",
    "neighbors = leven_index.get_nns_by_string(query, topk=3)\n",
    "\n",
    "print(\"Top 3 nearest neighbors:\")\n",
    "for idx in neighbors:\n",
    "    print(f\"[{idx}] {leven_index.get_strings()[idx]}\")\n",
    "\n",
    "# save\n",
    "leven_index.save(\"Models/LevenshteinIndex_10k_2.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LevenshteinForest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('Data/Random_strings_10k.txt', delimiter='\\t', encoding='utf-8')\n",
    "train_strings = data['String']\n",
    "\n",
    "forest = LevenshteinForest(n_estimators=10, max_depth=150)\n",
    "forest.fit(train_strings)\n",
    "\n",
    "# codes = forest.transform(train_strings)\n",
    "predictions = forest.predict([\"new string\"], topk=3)\n",
    "print(predictions)\n",
    "\n",
    "forest.save(\"Models/LevenshteinForest_10k.pkl\")\n",
    "# loaded = LevenshteinForest.load(\"Models/LevenshteinForest_10k.pkl\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_ANN_Leven",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
